set_engine("glmnet")
lasso_model
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
head(data)
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
sparse_df = data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25) %>%
ungroup() %>%
cast_sparse(row = score, column = word, value = n)
sparse_df
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
data %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ")%>%
count(score,word) %>%
anti_join(stop_words) %>%
filter(n>=3) %>%
bind_tf_idf(word,score,n) %>%
group_by(score) %>%
top_n(tf_idf, n = 5) %>%
ungroup() %>%
mutate(score = as.factor(score)) %>%
ggplot(aes(x = reorder_within(word, tf_idf, score), y = tf_idf, fill = score)) + geom_col() + scale_x_reordered() + coord_flip()+facet_wrap(~score, scales = 'free')+ theme(legend.position = 'none')
library(tidymodels)
tidy_data = data %>% select(-username)
tidy_split = initial_split (tidy_data, p =.8)
tidy_train = training(tidy_split)
tidy_test = testing(tidy_split)
library(textrecipes)
library(stopwords)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
cross_validation = vfold_cv(tidy_train, v = 10, repeats = 10)
cross_validation
wf = workflow() %>%
add_recipe(text_recipe)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty, levels = 10)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
install.packages('glmnet')
library(glmnet)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
library(tidymodels)
tidy_data = data %>% select(-username)
tidy_split = initial_split (tidy_data, p =.8)
tidy_train = training(tidy_split)
tidy_test = testing(tidy_split)
library(textrecipes)
library(stopwords)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
cross_validation = vfold_cv(tidy_train, v = 10, repeats = 10)
cross_validation
wf = workflow() %>%
add_recipe(text_recipe)
library(textrecipes)
library(stopwords)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text, token = stringr::str_split,pattern = " ") %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text, token = stringr::str_split,pattern = " ")
View(text_recipe)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text, token = stringr::str_split,pattern = " ") %>%
step_stopwords(review_text) %>%
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text, token = stringr::str_split,pattern = " ") %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200)
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
sparse_df = data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25) %>%
ungroup() %>%
cast_sparse(row = score, column = word, value = n)
sparse_df
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
data %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ")%>%
count(score,word) %>%
anti_join(stop_words) %>%
filter(n>=3) %>%
bind_tf_idf(word,score,n) %>%
group_by(score) %>%
top_n(tf_idf, n = 5) %>%
ungroup() %>%
mutate(score = as.factor(score)) %>%
ggplot(aes(x = reorder_within(word, tf_idf, score), y = tf_idf, fill = score)) + geom_col() + scale_x_reordered() + coord_flip()+facet_wrap(~score, scales = 'free')+ theme(legend.position = 'none')
library(tidymodels)
tidy_data = data %>% select(-username)
tidy_split = initial_split (tidy_data, p =.8)
tidy_train = training(tidy_split)
tidy_test = testing(tidy_split)
View(data)
library(textrecipes)
library(stopwords)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text, label = 'review_text',token = stringr::str_split,pattern = " ") %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
library(textrecipes)
library(stopwords)
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
cross_validation = vfold_cv(tidy_train, v = 10, repeats = 10)
cross_validation
wf = workflow() %>%
add_recipe(text_recipe)
text_prep
library(glmnet)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
library(textrecipes)
library(stopwords)
text_recipe = recipe(as.factor(score)~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
library(textrecipes)
library(stopwords)
tidy_train %>%
mutate(score = as.factor(score))
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
text_prep
cross_validation = vfold_cv(tidy_train, v = 10, repeats = 10)
cross_validation
wf = workflow() %>%
add_recipe(text_recipe)
library(glmnet)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
library(textrecipes)
library(stopwords)
tidy_train %>%
mutate(score = as.factor(score)) %>%
mutate(drop_na())
library(textrecipes)
library(stopwords)
tidy_train %>%
mutate(score = as.factor(score)) %>%
mutate(drop_na(score))
View(tidy_train)
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data %>%
mutate(drop_na())
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data %>%
mutate(drop_na)
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data %>%
drop_na()
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
data = read.csv('fall_guy_reviews.csv')
data %>%
drop_na()
View(data)
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data = data[,-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233,)]
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data = data[,-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233)]
View(data)
data = read.csv('fall_guy_reviews.csv')
data = data[,-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233)]
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data = data[-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233)]
data = read.csv('fall_guy_reviews.csv')
data = data[-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233),]
View(data)
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data = data[-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233),]
head(data)
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
sparse_df = data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25) %>%
ungroup() %>%
cast_sparse(row = score, column = word, value = n)
sparse_df
library(irlba)
pca_text = prcomp_irlba(sparse_df, n =4 , scale = TRUE)
pca_text
pca_text$center %>%
tidy() %>%
select(names) %>%
cbind(pca_text$rotation) %>%
ggplot(aes(x=PC1, y = PC2, label = names)) + geom_point()+geom_text()
data %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ")%>%
count(score,word) %>%
anti_join(stop_words) %>%
filter(n>=3) %>%
bind_tf_idf(word,score,n) %>%
group_by(score) %>%
top_n(tf_idf, n = 5) %>%
ungroup() %>%
mutate(score = as.factor(score)) %>%
ggplot(aes(x = reorder_within(word, tf_idf, score), y = tf_idf, fill = score)) + geom_col() + scale_x_reordered() + coord_flip()+facet_wrap(~score, scales = 'free')+ theme(legend.position = 'none')
library(tidymodels)
tidy_data = data %>% select(-username)
tidy_split = initial_split (tidy_data, p =.8)
tidy_train = training(tidy_split)
tidy_test = testing(tidy_split)
library(textrecipes)
library(stopwords)
tidy_train %>%
mutate(score = as.factor(score))
text_recipe = recipe(score~review_text, data = tidy_train) %>%
step_tokenize(review_text) %>%
step_stopwords(review_text) %>%
step_tokenfilter(review_text, max_tokens = 200) %>%
step_tf(review_text)
text_prep = text_recipe %>% prep()
text_prep
cross_validation = vfold_cv(tidy_train, v = 10, repeats = 10)
cross_validation
wf = workflow() %>%
add_recipe(text_recipe)
library(glmnet)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
lasso_tune %>%
pull(.metrics)
library(glmnet)
lasso_model = linear_reg(penalty = tune(), mixture = 1) %>%
set_mode("regression") %>%
set_engine("glmnet")
lasso_model
lasso_grid = grid_regular(penalty(), levels = 10)
lasso_tune = tune_grid(
wf %>%
add_model(lasso_model),
resamples = cross_validation,
grid = lasso_grid
)
lasso_tune %>%
collect_metrics() %>%
ggplot(aes(x=penalty, y =mean)) +geom_line() + facet_wrap(~.metrics, scales = 'free')
lasso_tune %>%
collect_metrics()
lasso_tune %>%
collect_metrics() %>%
ggplot(aes(x=penalty, y =mean)) +geom_line() + facet_wrap(~.metric, scales = 'free')
lasso_tune %>% select_best('rmse')
lasso_tune %>%
collect_metrics() %>%
ggplot(aes(x=penalty, y =mean)) +geom_line() + facet_wrap(~.metric, scales = 'free')
lasso_tune %>% select_best('rmse')
lasso_best_tune = lasso_tune %>% select_best('rmse')
final_lasso_model = finalize_model(lasso_model, lasso_best_tune)
lasso_wf = workflow() %>% add_recipe(text_recipe) %>% add_model(final_lasso_model)
lasso_eval = lasso_wf %>% last_fit(tidy_split)
lasso_eval %>% collect_metrics()
#Graph of the distribution of scores
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score)
data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25)
data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25) %>%
ungroup()
knitr::opts_chunk$set(echo = TRUE)
#Read data and drops rows that had strange usernames or Na scores.
library(tidyverse)
library(tidytext)
data = read.csv('fall_guy_reviews.csv')
data = data[-c(75,76,77,239,312,313,314,315,316,317,318,319,340,162,57,320,242,165,78,240,221,341,85,170,233),]
head(data)
#Graph of the distribution of scores
#Fall guys seems to have good reviews
data %>%
count(score) %>%
ggplot(aes(x=score, y = n)) + geom_col()
#Vocab:
#Sparse DF: Seems to just be a df that reduces memory usage. Need to do more research.
#unnest_tokens = creates a column (word) and splits up review text by spaces in the text.
#it stores that and then does a word count on unique words per score.
#it then filters out so only words that appear >= 5 times per score show
#then groups that by the scores so we can take the top 25 words by score only.
#
sparse_df = data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25) %>%
ungroup() %>%
cast_sparse(row = score, column = word, value = n)
sparse_df
data %>%
select(score,review_text) %>%
unnest_tokens(word, review_text, token = stringr::str_split, pattern = " ") %>%
count(score,word) %>%
filter(n>=5) %>%
group_by(score) %>%
top_n(n,n=25)
